{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d17c4e97a6ae6795ba0994e13f4557cf2dcf6d13"
   },
   "source": [
    "# Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Bidirectional, GlobalMaxPool1D, Activation, CuDNNLSTM\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.metrics import accuracy_score\n",
    "## Parameters\n",
    "maxwords_question = 100\n",
    "maxlen_word = 300 \n",
    "maxwords_vocabulary = 50000 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "db4e2396844b8ac72a74f52190b5f9857a9597b3"
   },
   "source": [
    "# Load and preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "b47c8115d826a5aaa7ed8b2ca0504e25657a4b91",
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_df = pd.read_csv(\"../input/train.csv\")\n",
    "\n",
    "# Split into train and test\n",
    "train_df, test_df = train_test_split(data_df, test_size = 0.1, random_state=55)\n",
    "\n",
    "## Lower case all alphabets\n",
    "train_df[\"processed_text\"] = train_df[\"question_text\"].apply(lambda x: x.lower())\n",
    "test_df[\"processed_text\"] = test_df[\"question_text\"].apply(lambda x: x.lower())\n",
    "\n",
    "## Tokenizer -- turns each question into a sequence of integers (each integer being the index of a token in a dictionary)\n",
    "tokenizer = Tokenizer(num_words = maxwords_vocabulary)\n",
    "tokenizer.fit_on_texts(list(train_df[\"processed_text\"]) + list(test_df[\"processed_text\"]))\n",
    "\n",
    "# Tokenize train and test data\n",
    "train_df_tokens = tokenizer.texts_to_sequences(train_df[\"processed_text\"])\n",
    "train_df_labels = train_df[\"target\"].values\n",
    "test_df_tokens = tokenizer.texts_to_sequences(test_df[\"processed_text\"])\n",
    "\n",
    "## Pad the tokenized sequences to length maxwords_question\n",
    "train_df_tokens = pad_sequences(train_df_tokens, maxlen = maxwords_question)\n",
    "test_df_tokens = pad_sequences(test_df_tokens, maxlen = maxwords_question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bc40afbbb14295f4c487d74f628db56e21a256fc"
   },
   "source": [
    "# Create validation dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b88ec35096da4a9e01745f3ba04e3dc3530baaff",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Create and tokenize validation dataset\n",
    "train_df2, val_df2 = train_test_split(train_df, test_size=0.05, random_state=42)\n",
    "print(\"======Train data========\")\n",
    "print(train_df2.target.value_counts())\n",
    "train_df2_tokens = tokenizer.texts_to_sequences(train_df2[\"processed_text\"])\n",
    "train_df2_labels = train_df2[\"target\"].values\n",
    "print(\"====Validation data=======\")\n",
    "print(val_df2.target.value_counts())\n",
    "val_df2_tokens = tokenizer.texts_to_sequences(val_df2[\"processed_text\"])\n",
    "val_df2_labels = val_df2[\"target\"].values\n",
    "val_df2.head()\n",
    "\n",
    "## Pad the tokenized sequences to length maxwords_question\n",
    "train_df2_tokens = pad_sequences(train_df2_tokens, maxlen = maxwords_question)\n",
    "val_df2_tokens = pad_sequences(val_df2_tokens, maxlen = maxwords_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5321f204fb0d5ff2460b0b697bf005c9729531c7"
   },
   "source": [
    "# Load GloVe Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "b827b48caff6e5235b670a0a73004ba6e6fa4546"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load GloVe embeddings from disk\n",
    "embeddingGlovePath = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "def split_key_value(item): \n",
    "    key, *value = item.split(\" \")\n",
    "    return key, np.asarray(value, dtype='float32')\n",
    "embeddings_dict = {}\n",
    "for item in open(embeddingGlovePath):\n",
    "    key, value = split_key_value(item)\n",
    "    embeddings_dict[key] = value\n",
    "\n",
    "## Make embeddings matrix from the loaded GloVe embeddings\n",
    "word_index = tokenizer.word_index\n",
    "glove_embs = np.stack(embeddings_dict.values())\n",
    "glove_embs_mean, glove_embs_std = glove_embs.mean(), glove_embs.std()\n",
    "glove_embs_size = glove_embs.shape[1]\n",
    "del glove_embs\n",
    "glove_embs_matrix = np.random.normal(glove_embs_mean, glove_embs_std, (maxwords_vocabulary, glove_embs_size))\n",
    "for word, idx in word_index.items():\n",
    "    if idx >= maxwords_vocabulary: \n",
    "        continue\n",
    "    if word in embeddings_dict.keys():\n",
    "        word_emb_vector = embeddings_dict[word]\n",
    "    if word_emb_vector is not None: \n",
    "        glove_embs_matrix[idx] = word_emb_vector\n",
    "del embeddings_dict\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1e325d936e5437ab016177ca06369c0701f244a2"
   },
   "source": [
    "# Bi-directional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "e88535ad66a09cdd9eea4455cb3962e915c32555"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 100, 300)          15000000  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 256)               440320    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 520       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 15,457,297\n",
      "Trainable params: 15,457,297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "inp = Input(shape = (maxwords_question,) )\n",
    "x = Embedding(maxwords_vocabulary, maxlen_word, weights=[glove_embs_matrix])(inp)\n",
    "x = Bidirectional(CuDNNLSTM(128, return_sequences=False))(x)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "x = Dense(8, activation=\"relu\")(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs = inp, outputs = x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d2f544ae6cb287ae59b5135ebbc7cb56fa2b8ba2"
   },
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "23bd4740b8d922bf9c5c6d0791f40ac56cfccf26"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1175509/1175509 [==============================] - 429s 365us/step - loss: 0.1104 - acc: 0.9564\n",
      "Epoch 2/2\n",
      "1175509/1175509 [==============================] - 427s 363us/step - loss: 0.0897 - acc: 0.9639\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe2c4bce828>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_df_tokens, train_df_labels, batch_size = 256, epochs=2, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c652ff82108ab7c556bd6af04e940b7701802241"
   },
   "source": [
    "# Predict test labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "96456822ff7d9939dff23449d103ce74238103ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130613/130613 [==============================] - 11s 84us/step\n",
      "Mean Accuracy (test data):  0.9576841508885027\n",
      "F1 score insincere questions (test data):  0.676348304737366\n"
     ]
    }
   ],
   "source": [
    "test_pred = model.predict([test_df_tokens], batch_size=512, verbose=1) > 0.35\n",
    "print('Mean Accuracy (test data): ', accuracy_score(test_pred, test_df.target))\n",
    "print('F1 score insincere questions (test data): ',f1_score(test_pred, test_df.target, pos_label=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3834d058176b573f74744f63ad4ed0b3900886e7",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
